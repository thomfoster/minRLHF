import torch
from torch.nn.functional import pad
from typing import Optional, Iterable, Dict

def logical_or_without_broadcasting(x, y):
    """Right pads x to be same size as y in last dimension, then computes logical or.
    
    Frequently used to pad the input_mask...
        (input_mask[i,j] = 1 if the jth token of the ith example is part of the input prompt, 0 otherwise)
    
    ...to be the same dimension as the output_mask...
        (output_mask[i,j] = 1 if the jth token of the ith example was generated by the model, 0 otherwise)
    
    ...and therefore the logical or computes the full mask
        (full_mask[i,j] = 1 if the jth token of the ith example is part of the full output text.
    """
    
    assert x.shape[:-1] == y.shape[:-1]
    input_length = x.shape[1]
    output_length = y.shape[1]
    padding_amount = output_length - input_length
    assert padding_amount >= 0
    padded_x = pad(x, (0, padding_amount))
    
    return torch.logical_or(padded_x, y)


def gather_dict(d: Dict[str,torch.Tensor], device: torch.device, keys: Optional[Iterable[str]]=None):
    """Move the tensors at the given keys to device."""
    if keys is None:
        keys = d.keys()
    for key in keys:
        d[key] = d[key].to(device)
    return d
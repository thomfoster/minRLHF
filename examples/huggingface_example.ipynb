{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import minRLHF\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from minRLHF.environment import Environment\n",
    "import random\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "reward_model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model='bhadresh-savani/distilbert-base-uncased-emotion', \n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "class MyEnv(Environment):\n",
    "    def get_input_prompt(self) -> str:\n",
    "        return random.choice([\n",
    "            'I went for a walk one day and',\n",
    "            'A long time ago, in a galaxy far far away',\n",
    "            'Oops! I'\n",
    "        ])\n",
    "        \n",
    "    def score_generation(self, text: str) -> float:\n",
    "        sentiment_scores = reward_model(text)[0]\n",
    "        sentiment_scores = {d['label']: d['score'] for d in sentiment_scores}\n",
    "        return sentiment_scores['joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'h.0.attn.masked_bias', 'classifier.weight', 'h.11.attn.masked_bias', 'h.6.attn.masked_bias', 'h.5.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.4.attn.masked_bias', 'h.1.attn.masked_bias', 'h.8.attn.masked_bias', 'h.2.attn.masked_bias', 'h.7.attn.masked_bias', 'h.3.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').to('cuda')\n",
    "reference = AutoModelForCausalLM.from_pretrained('gpt2').to('cuda')\n",
    "critic = AutoModelForTokenClassification.from_pretrained('gpt2', num_labels=1).to('cuda')\n",
    "\n",
    "# Instantiate envrionment\n",
    "env = MyEnv(tokenizer, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minRLHF.ppo_trainer import PPOTrainer\n",
    "\n",
    "# Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    actor_model=model,\n",
    "    critic_model=critic,\n",
    "    reference_model=reference,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rollout batch 0\n",
      "Generating rollout batch 1\n",
      "Generating rollout batch 2\n",
      "Generating rollout batch 3\n",
      "Generating rollout batch 4\n",
      "Generating rollout batch 5\n",
      "Generating rollout batch 6\n",
      "Generating rollout batch 7\n",
      "Generating rollout batch 8\n",
      "Generating rollout batch 9\n",
      "Getting actor loss for train step 0 and batch 0\n",
      "Getting actor loss for train step 0 and batch 1\n",
      "Getting actor loss for train step 0 and batch 2\n",
      "Getting actor loss for train step 0 and batch 3\n",
      "Getting actor loss for train step 0 and batch 4\n",
      "Getting actor loss for train step 0 and batch 5\n",
      "Getting actor loss for train step 0 and batch 6\n",
      "Getting actor loss for train step 0 and batch 7\n",
      "Getting actor loss for train step 0 and batch 8\n",
      "Getting actor loss for train step 0 and batch 9\n",
      "Getting actor loss for train step 0 and batch 10\n",
      "Getting actor loss for train step 0 and batch 11\n",
      "Getting actor loss for train step 0 and batch 12\n",
      "Getting actor loss for train step 0 and batch 13\n",
      "Getting actor loss for train step 0 and batch 14\n",
      "Getting actor loss for train step 0 and batch 15\n",
      "Getting actor loss for train step 0 and batch 16\n",
      "Getting actor loss for train step 0 and batch 17\n",
      "Getting actor loss for train step 0 and batch 18\n",
      "Getting actor loss for train step 0 and batch 19\n",
      "Getting actor loss for train step 1 and batch 0\n",
      "Getting actor loss for train step 1 and batch 1\n",
      "Getting actor loss for train step 1 and batch 2\n",
      "Getting actor loss for train step 1 and batch 3\n",
      "Getting actor loss for train step 1 and batch 4\n",
      "Getting actor loss for train step 1 and batch 5\n",
      "Getting actor loss for train step 1 and batch 6\n",
      "Getting actor loss for train step 1 and batch 7\n",
      "Getting actor loss for train step 1 and batch 8\n",
      "Getting actor loss for train step 1 and batch 9\n",
      "Getting actor loss for train step 1 and batch 10\n",
      "Getting actor loss for train step 1 and batch 11\n",
      "Getting actor loss for train step 1 and batch 12\n",
      "Getting actor loss for train step 1 and batch 13\n",
      "Getting actor loss for train step 1 and batch 14\n",
      "Getting actor loss for train step 1 and batch 15\n",
      "Getting actor loss for train step 1 and batch 16\n",
      "Getting actor loss for train step 1 and batch 17\n",
      "Getting actor loss for train step 1 and batch 18\n",
      "Getting actor loss for train step 1 and batch 19\n",
      "Early stopping at 1 due to kl of ~ 0.16519638784229757\n",
      "Getting critic loss for step 0 and batch 0\n",
      "Getting critic loss for step 0 and batch 1\n",
      "Getting critic loss for step 0 and batch 2\n",
      "Getting critic loss for step 0 and batch 3\n",
      "Getting critic loss for step 0 and batch 4\n",
      "Getting critic loss for step 0 and batch 5\n",
      "Getting critic loss for step 0 and batch 6\n",
      "Getting critic loss for step 0 and batch 7\n",
      "Getting critic loss for step 0 and batch 8\n",
      "Getting critic loss for step 0 and batch 9\n",
      "Getting critic loss for step 0 and batch 10\n",
      "Getting critic loss for step 0 and batch 11\n",
      "Getting critic loss for step 0 and batch 12\n",
      "Getting critic loss for step 0 and batch 13\n",
      "Getting critic loss for step 0 and batch 14\n",
      "Getting critic loss for step 0 and batch 15\n",
      "Getting critic loss for step 0 and batch 16\n",
      "Getting critic loss for step 0 and batch 17\n",
      "Getting critic loss for step 0 and batch 18\n",
      "Getting critic loss for step 0 and batch 19\n",
      "Getting critic loss for step 1 and batch 0\n",
      "Getting critic loss for step 1 and batch 1\n",
      "Getting critic loss for step 1 and batch 2\n",
      "Getting critic loss for step 1 and batch 3\n",
      "Getting critic loss for step 1 and batch 4\n",
      "Getting critic loss for step 1 and batch 5\n",
      "Getting critic loss for step 1 and batch 6\n",
      "Getting critic loss for step 1 and batch 7\n",
      "Getting critic loss for step 1 and batch 8\n",
      "Getting critic loss for step 1 and batch 9\n",
      "Getting critic loss for step 1 and batch 10\n",
      "Getting critic loss for step 1 and batch 11\n",
      "Getting critic loss for step 1 and batch 12\n",
      "Getting critic loss for step 1 and batch 13\n",
      "Getting critic loss for step 1 and batch 14\n",
      "Getting critic loss for step 1 and batch 15\n",
      "Getting critic loss for step 1 and batch 16\n",
      "Getting critic loss for step 1 and batch 17\n",
      "Getting critic loss for step 1 and batch 18\n",
      "Getting critic loss for step 1 and batch 19\n",
      "Completed epoch 0.\n",
      "reward_mean: 0.4620726704597473 (0.4620726704597473 average)\n",
      "reward_std: 0.4449054002761841 (0.4449054002761841 average)\n",
      "augmented_reward: 0.4620726704597473 (0.4620726704597473 average)\n",
      "completion_length_mean: 87.265625 (87.265625 average)\n",
      "completion_length_std: 10.256953239440918 (10.256953239440918 average)\n",
      "kld_t-1: 0.16519638784229757 (0.16519638784229757 average)\n",
      "kld_0: 0.16519638784229757 (0.16519638784229757 average)\n",
      "entropy: 3.4319066405296326 (3.4319066405296326 average)\n",
      "\n",
      "Generating rollout batch 0\n",
      "Generating rollout batch 1\n",
      "Generating rollout batch 2\n",
      "Generating rollout batch 3\n",
      "Generating rollout batch 4\n",
      "Generating rollout batch 5\n",
      "Generating rollout batch 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/minRLHF/minRLHF/ppo_trainer.py:198\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=195'>196</a>\u001b[0m \u001b[39mfor\u001b[39;00m rollout_batch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_batches_per_epoch):\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=196'>197</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGenerating rollout batch \u001b[39m\u001b[39m{\u001b[39;00mrollout_batch_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=197'>198</a>\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_rollout()\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=198'>199</a>\u001b[0m     rollout \u001b[39m=\u001b[39m gather_dict(rollout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mstore(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrollout)\n",
      "File \u001b[0;32m~/minRLHF/minRLHF/ppo_trainer.py:162\u001b[0m, in \u001b[0;36mPPOTrainer.get_rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=159'>160</a>\u001b[0m \u001b[39m# Completions and associated logprobs computed on actor device\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=160'>161</a>\u001b[0m data \u001b[39m=\u001b[39m gather_dict(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mdevice, keys\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mprompt_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprompt_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=161'>162</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mcompletion_ids\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mcompletion_mask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mget_rollouts(data[\u001b[39m'\u001b[39;49m\u001b[39mprompt_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mprompt_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=162'>163</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mpi_t_logprobs\u001b[39m\u001b[39m'\u001b[39m], _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mget_logits(data[\u001b[39m'\u001b[39m\u001b[39mcompletion_ids\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mprompt_mask\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mcompletion_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=164'>165</a>\u001b[0m \u001b[39m# Reference logprobs computed on reference device\u001b[39;00m\n",
      "File \u001b[0;32m~/minRLHF/minRLHF/actor.py:48\u001b[0m, in \u001b[0;36mActor.get_rollouts\u001b[0;34m(self, input_ids, input_mask)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=37'>38</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=38'>39</a>\u001b[0m \u001b[39mArgs: \u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=39'>40</a>\u001b[0m \u001b[39m    input_ids: Tensor(batch, input_seq_len), \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=44'>45</a>\u001b[0m \u001b[39m    output_mask: Tensor(batch, output_seq_len)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=45'>46</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=46'>47</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=47'>48</a>\u001b[0m     output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=48'>49</a>\u001b[0m         input_ids,\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=49'>50</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minput_mask,\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=50'>51</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=51'>52</a>\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_during_generation,\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=52'>53</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_temperature,\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=53'>54</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_max_length\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=56'>57</a>\u001b[0m \u001b[39m# pad output_ids to be (batch, max_length) in case all completions stopped early\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/minRLHF/actor.py?line=57'>58</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_max_length \u001b[39m-\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py:1543\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1534'>1535</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1535'>1536</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1536'>1537</a>\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1537'>1538</a>\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1538'>1539</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1539'>1540</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1541'>1542</a>\u001b[0m     \u001b[39m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1542'>1543</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1543'>1544</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1544'>1545</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1545'>1546</a>\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1546'>1547</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1547'>1548</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1548'>1549</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1549'>1550</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1550'>1551</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1551'>1552</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1552'>1553</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1553'>1554</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1555'>1556</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=1556'>1557</a>\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py:2482\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2478'>2479</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2480'>2481</a>\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2481'>2482</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2482'>2483</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2483'>2484</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2484'>2485</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2485'>2486</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2486'>2487</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2488'>2489</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/generation_utils.py?line=2489'>2490</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1037'>1038</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1041'>1042</a>\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1042'>1043</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1045'>1046</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1058'>1059</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1059'>1060</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1060'>1061</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1062'>1063</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=878'>879</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=879'>880</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=880'>881</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=885'>886</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=896'>897</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=899'>900</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=900'>901</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=386'>387</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=387'>388</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=388'>389</a>\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=389'>390</a>\u001b[0m     hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=390'>391</a>\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=391'>392</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=392'>393</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=393'>394</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=394'>395</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=395'>396</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=396'>397</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=397'>398</a>\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:320\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=317'>318</a>\u001b[0m     past_key, past_value \u001b[39m=\u001b[39m layer_past\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=318'>319</a>\u001b[0m     key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((past_key, key), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=319'>320</a>\u001b[0m     value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((past_value, value), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=321'>322</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=322'>323</a>\u001b[0m     present \u001b[39m=\u001b[39m (key, value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = list(range(len(ppo_trainer.rolling_rewards)))\n",
    "ys = ppo_trainer.rolling_rewards\n",
    "\n",
    "window_size = 10\n",
    "smoothed_ys = [sum(ys[max(0, idx-window_size):idx])/window_size for idx, _ in enumerate(ys)]\n",
    "\n",
    "plt.scatter(xs, ys, s=1)\n",
    "plt.plot(smoothed_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('I went for a walk one day and', return_tensors='pt')\n",
    "outputs = reference.generate(inputs.input_ids.to(model.device), max_length=100, do_sample=True)\n",
    "text = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('../actor_649.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no. Here's our third party.\n",
      "\n",
      "\"We're here because you want to. And here's your option.\n",
      "\n",
      "\"We wanted to go with your name here because you've got it on file with ICE, and ICE's doing that very brilliantly with ICE's investigatory-immigration program.\n",
      "\n",
      "\"And ICE's got all of that up.\n",
      "\n",
      "\"So, if all of that's up, you're not here, and ICE's got all of that up\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('Oh no', return_tensors='pt')\n",
    "outputs = model.generate(inputs.input_ids, do_sample=True, max_length=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f08a5f13ca9247f60687871aeb0db0c250749e5b174701901907dc6c613fa60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

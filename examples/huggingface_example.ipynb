{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import minRLHF\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minRLHF/.venv/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from minRLHF.environment import Environment\n",
    "import random\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "reward_model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model='bhadresh-savani/distilbert-base-uncased-emotion', \n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "class MyEnv(Environment):\n",
    "    def get_input_prompt(self) -> str:\n",
    "        return random.choice([\n",
    "            'I went for a walk one day and',\n",
    "            'A long time ago, in a galaxy far far away',\n",
    "            'Oops! I'\n",
    "        ])\n",
    "        \n",
    "    def score_generation(self, text: str) -> float:\n",
    "        sentiment_scores = reward_model(text)[0]\n",
    "        sentiment_scores = {d['label']: d['score'] for d in sentiment_scores}\n",
    "        return sentiment_scores['joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.5.attn.masked_bias', 'h.3.attn.masked_bias', 'h.10.attn.masked_bias', 'h.9.attn.masked_bias', 'h.6.attn.masked_bias', 'h.0.attn.masked_bias', 'h.11.attn.masked_bias', 'classifier.bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.4.attn.masked_bias', 'h.2.attn.masked_bias', 'h.1.attn.masked_bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').to('cuda')\n",
    "reference = AutoModelForCausalLM.from_pretrained('gpt2').to('cuda')\n",
    "critic = AutoModelForTokenClassification.from_pretrained('gpt2', num_labels=1).to('cuda')\n",
    "\n",
    "# Instantiate envrionment\n",
    "env = MyEnv(tokenizer, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minRLHF.ppo_trainer import PPOTrainer\n",
    "\n",
    "# Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    actor_model=model,\n",
    "    critic_model=critic,\n",
    "    reference_model=reference,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/minRLHF/minRLHF/ppo_trainer.py:198\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=194'>195</a>\u001b[0m     rollout \u001b[39m=\u001b[39m gather_dict(rollout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=195'>196</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mstore(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrollout)\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=197'>198</a>\u001b[0m buf_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgamma, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlam, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta) \u001b[39m# TODO! implement a buffer.get_batches()    Since we'll need to iterate over data twice do this as a Map Dataset not iterable. Dunno how we'll store completions of differing lengths :O\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=199'>200</a>\u001b[0m \u001b[39m# Use the rollouts to optimise the actor\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/ppo_trainer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m actor_train_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_train_iters):\n",
      "File \u001b[0;32m~/minRLHF/minRLHF/buffer.py:120\u001b[0m, in \u001b[0;36mBuffer.get\u001b[0;34m(self, gamma, lam, beta)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=106'>107</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=107'>108</a>\u001b[0m \u001b[39mCompute advantages etc, then return tensors.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=108'>109</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=111'>112</a>\u001b[0m \u001b[39m         info: dict[str: float] - extra info, like perplexity, kld from \u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=112'>113</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=113'>114</a>\u001b[0m \u001b[39m# TODO: Extend with \"state by state\" style return for use with standard libraries.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=114'>115</a>\u001b[0m \u001b[39m# TODO: Add batch_size parameter for iterated return of data, to allow gradient accumulation.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=115'>116</a>\u001b[0m \u001b[39m# TODO: Extend for arbitrary augmentation function.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=116'>117</a>\u001b[0m \u001b[39m# TODO: Add returnd device map\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=117'>118</a>\u001b[0m \u001b[39m# TODO: Add `last_val` option for controlling end of sequence rewards.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=119'>120</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptr \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_episodes    \u001b[39m# Asserting full makes tensor logic much easier\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/ubuntu/minRLHF/minRLHF/buffer.py?line=122'>123</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_augmenter(\u001b[39mself\u001b[39m)     \u001b[39m# Side effect: Fills augmentation buffer\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f08a5f13ca9247f60687871aeb0db0c250749e5b174701901907dc6c613fa60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
